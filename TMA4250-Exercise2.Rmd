--- 
title: "TMA4250 Spatial Statistics Exercise 1, Spring 2019"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output:
  pdf_document: default
  bookdown::html_document2: default
  html_document:
    df_print: paged
subtitle: 'Group members: Henrik Syversveen Lie, Ã˜yvind Auestad'
#references:
#- id: Omre
#  title: Bayesian Spatial Inversion with Conjugate Prior Models
#  author:
#  - family: Omre
#    given: Henning
#  type: article-journal
#  issued:
#    year: 2019
#    month: 1
---


```{r setup, include = FALSE}
library(bookdown)
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```


```{r, echo = F, eval = T}
library(reshape2)
library(geoR)
library(ggplot2)
library(MASS)
library(cowplot)
library(fields)
library(akima)
library(spatial)
```

# Problem 1: Analysis of Point Patterns
## a)
We consider the three real data point patterns found in the R package `MASS`.
```{r, echo = F, eval = T, out.width = "33%"}
library(MASS)
data_cells <- ppinit("cells.dat")
data_redwood <- ppinit("redwood.dat")
data_pines <- ppinit("pines.dat")

plot(data_cells, main = "Data from cells.dat file", xlab = "x", ylab = "y")
plot(data_redwood, main = "Data from redwood.dat file",xlab = "x", ylab = "y")
plot(data_pines, main = "Data from pines.dat file",xlab = "x", ylab = "y")
```

The cell data in the left plot represents some cell structure. Here, the cells have a repulsive effect on each other, so two cells are not likely to be close to each other. Thus, we get a point pattern that looks to be repulsive. The middle plot represents locations of redwood trees. The trees release seeds, which fall to the ground close to them, so trees get clustered together around a "mother tree". We therefore get a point pattern that contains clusters of points. Finally, the plot to the right is locations of pine trees. Presumably, seeds from these trees can get carried further away from the tree trunk by e.g. animals eating the pinecones. Hence, the location of one tree is independent of locations of other trees, and we get a pattern that looks like a stationary Poisson random field.

## b)
We then want to compute the empirical $L$-function for each of the point patterns, and use the R function `Kfn()` to do this. The $L$-functions are then displayed for each point pattern.

```{r, echo = F, eval = T, out.width = "33%"}
ppinit("cells.dat")
cells_L <- Kfn(data_cells,0.7)

ppinit("redwood.dat")
redwood_L <- Kfn(data_redwood, 0.7)

ppinit("pines.dat")
pines_L <- Kfn(data_pines,7)

plot(cells_L, type="p", xlab="distance", ylab="L(t)", main = "L-function for cells data")

plot(redwood_L, type="p", xlab="distance", ylab="L(t)", main = "L-function for redwood data")

plot(pines_L, type="p", xlab="distance", ylab="L(t)", main = "L-function for pines data")
```
The plots of the $L$-functions are closely related to the displayed point patterns. The cells data has a low value of the $L$-function for low values of $t$. This means that there is a low probability of points being close to each other, we have a repulsive model, which is verified on the displayed point pattern. The redwood data on the other hand, has a high value of the $L$-function for low values of $t$. This means that there is a large probability of points being close, we have a clustered model, which also is verified on the displayed point pattern. Finally we have the pines data, for which the $L$-function closely resembles a straight line. This means that the point pattern is similar to a stationary Poisson RF.

The interaction function $J(t)$ is defined as,
$$J(t) = \frac{\mathbf{E}[k_{\mathbf{B}_\mathbf{x_0}(t)}-1]}{|\mathbf{B}_{\mathbf{x_0}}\cap D|}; t\in \mathbb{R}_\oplus,$$
where $\mathbf{B}_{\mathbf{x_0}}(t)$ is defined as the ball with radius $t$ and center $\mathbf{x_0}\in \mathbb{X}_D$. For a stationary Poisson random field, one gets,
$$J(t) = \frac{\lambda_k |\mathbf{B}_{\mathbf{x_0}}\cap D|}{|\mathbf{B}_{\mathbf{x_0}}\cap D|} = \lambda_k.$$
Hence, for stationary Poisson RF, the interaction-function is independent of the distance $t$.

An alternative interaction function is the dimension dependent $L$-function, which in $\mathbb{R}^2$ is,
$$L_2(t) = \bigg[\frac{\mathbf{E}[k_{\mathbf{B}_\mathbf{x_0}(t)}-1]}{\lambda_k\pi}  \bigg]^{1/2} = J(t) \times \frac{t}{\lambda_k}.$$
For the stationary Poisson random field, we get $L_2(t) = t$.

We then display the empirical $L$-function for each point pattern with the theoretical funtion for a stationary Poisson RF.

```{r, echo = F, eval = T, out.width = "33%"}
df_cells = data.frame(x = cells_L$x, y = cells_L$y)
df_redwood = data.frame(x = redwood_L$x, y = redwood_L$y)
df_pines = data.frame(x = pines_L$x, y = pines_L$y)

ggplot() + geom_point(data = df_cells, aes(x = x, y = y, colour = "Empirical")) + geom_line(aes(x = seq(0,0.7,length.out = 100), y = seq(0,0.7,length.out = 100),colour="Theoretical")) + ylab("L(t)") + xlab("t") + ggtitle("Comparison of empirical and stationary theoretical L-function for Cells data")+ theme(plot.title = element_text(size=10))
ggplot() + geom_point(data = df_redwood, aes(x = x, y = y, colour = "Empirical")) + geom_line(aes(x = seq(0,0.7,length.out = 100), y = seq(0,0.7,length.out = 100),colour="Theoretical")) + ylab("L(t)") + xlab("t") + ggtitle("Comparison of empirical and stationary theoretical L-function for Redwood data")+ theme(plot.title = element_text(size=10))
ggplot() + geom_point(data = df_pines, aes(x = x, y = y, col = "Empirical")) + geom_line(aes(x = seq(0,7,length.out = 100), y = seq(0,7,length.out = 100),colour="Theoretical")) + ylab("L(t)") + xlab("t") + ggtitle("Comparison of empirical and stationary theoretical L-function for Pines data")+ theme(plot.title = element_text(size=10))
```
From the plots, we see that the empirical $L$-function for the Pines data is closest to the theoretical $L$-function of a stationary Poisson RF. The empirical $L$-function of the Redwood and Cells data have large deviations for small values of $t$, with Cells data having a smaller value, and Redwood data having a larger value than the theoretical stationary $L$-function. From the $L$-function and point pattern plots, we hypothesize that a stationary Poisson RF appear as a suitable model for the Pines data, but not for the two others.

## c)
Finally, we perform an empirical Monte Carlo test to check whether the stationary Poisson RF is a sutable model. We consider each point pattern - under the hypothesis that the point pattern origins from a stationary Poisson random field, condition on the observed point count, and generate 100 realizations of a stationary Poisson RF. For each realization we compute the $L$-function and use the 100 generated $L$-functions to empirically test whether the observed point pattern could origin from a stationary Poisson RF. To do this, we display empirical 0.9-intervals for the $L$-functions together with the estimated function for each point pattern.

```{r, echo = F, eval = T, out.width = "33%"}
k_cells <- length(data_cells$y)
k_redwood <- length(data_redwood$y)
k_pines <- length(data_pines$y)
L_matrix_cells <- matrix(NA, nrow = 100, ncol = length(cells_L$x))
L_matrix_redwood <- matrix(NA, nrow = 100, ncol = length(redwood_L$x))
L_matrix_pines <- matrix(NA, nrow = 100, ncol = length(pines_L$x))
for (i in 1:100){
  
  df_cells = data.frame(x = runif(k_cells,0,1), y = runif(k_cells,0,1))
  df_redwood = data.frame(x = runif(k_redwood,0,1), y = runif(k_redwood,-1,0))
  df_pines = data.frame(x = runif(k_pines,0,9.6), y = runif(k_pines,0,10))
  
  ppinit("cells.dat")
  L_matrix_cells[i,] = Kfn(df_cells,0.7)$y
  
  ppinit("redwood.dat")
  L_matrix_redwood[i,] = Kfn(df_redwood,0.7)$y
  
  ppinit("pines.dat")
  L_matrix_pines[i,] = Kfn(df_pines,7)$y
}

pred_interval_cells <- apply(L_matrix_cells,2,sort)
rownames(L_matrix_cells) = paste("trial", seq(100), sep="")
colnames(L_matrix_cells) = paste("x", seq(length(cells_L$x)), sep="")

dat_cells = as.data.frame(L_matrix_cells)
dat_cells$trial = rownames(L_matrix_cells)
mdat_cells = melt(dat_cells, id.vars="trial")
mdat_cells$x = as.numeric(gsub("x", "", mdat_cells$variable))/100*0.7
df_cells = data.frame(x = seq(0,0.7,length.out = 100), pred = cells_L$y, lb = pred_interval_cells[5,], ub = pred_interval_cells[95,])

ggplot() +
    theme_bw() +
    theme(panel.grid=element_blank()) +
    geom_line(data = mdat_cells, aes(x=x, y=value, group=trial), size=0.2, alpha=0.1) +
    geom_line(data = df_cells, aes(x = x, y = pred), size = 1, col = 'red') + 
    geom_line(data = df_cells, aes(x = x, y = lb), size = 1, col = 'blue', linetype = "dashed") + 
    geom_line(data = df_cells, aes(x = x, y = ub), size = 1, col = 'blue', linetype = "dashed") + ylab("L(t)") + xlab("t") + ggtitle("Empirical Monte Carlo test of stationarity for Cells data")

pred_interval_redwood <- apply(L_matrix_redwood,2,sort)
rownames(L_matrix_redwood) = paste("trial", seq(100), sep="")
colnames(L_matrix_redwood) = paste("x", seq(length(redwood_L$x)), sep="")

dat_redwood = as.data.frame(L_matrix_redwood)
dat_redwood$trial = rownames(L_matrix_redwood)
mdat_redwood = melt(dat_redwood, id.vars="trial")
mdat_redwood$x = as.numeric(gsub("x", "", mdat_redwood$variable))/100*0.7
df_redwood = data.frame(x = seq(0,0.7,length.out = 100), pred = redwood_L$y, lb = pred_interval_redwood[5,], ub = pred_interval_redwood[95,])

ggplot() +
    theme_bw() +
    theme(panel.grid=element_blank()) +
    geom_line(data = mdat_redwood, aes(x=x, y=value, group=trial), size=0.2, alpha=0.1) +
    geom_line(data = df_redwood, aes(x = x, y = pred), size = 1, col = 'red') + 
    geom_line(data = df_redwood, aes(x = x, y = lb), size = 1, col = 'blue', linetype = "dashed") + 
    geom_line(data = df_redwood, aes(x = x, y = ub), size = 1, col = 'blue', linetype = "dashed") + ylab("L(t)") + xlab("t") + ggtitle("Empirical Monte Carlo test of stationarity for Redwood data")

pred_interval_pines <- apply(L_matrix_pines,2,sort)
rownames(L_matrix_pines) = paste("trial", seq(100), sep="")
colnames(L_matrix_pines) = paste("x", seq(length(pines_L$x)), sep="")

dat_pines = as.data.frame(L_matrix_pines)
dat_pines$trial = rownames(L_matrix_pines)
mdat_pines = melt(dat_pines, id.vars="trial")
mdat_pines$x = as.numeric(gsub("x", "", mdat_pines$variable))*7/100
df_pines = data.frame(x = seq(0,7,length.out = 99), pred = pines_L$y, lb = pred_interval_pines[5,], ub = pred_interval_pines[95,])

ggplot() +
    theme_bw() +
    theme(panel.grid=element_blank()) +
    geom_line(data = mdat_pines, aes(x=x, y=value, group=trial), size=0.2, alpha=0.1) +
    geom_line(data = df_pines, aes(x = x, y = pred), size = 1, col = 'red') + 
    geom_line(data = df_pines, aes(x = x, y = lb), size = 1, col = 'blue', linetype = "dashed") + 
    geom_line(data = df_pines, aes(x = x, y = ub), size = 1, col = 'blue', linetype = "dashed") + ylab("L(t)") + xlab("t") + ggtitle("Empirical Monte Carlo test of stationarity for Pines data")
```
From the plots, we see that none of the point patterns are inside the 0.9-intervals for all values of $t$. Still, the Pines data is the one that is closest to the 0.9-interval. From both the plots of the point patterns and this test, we conclude that the first two (Cells and Redwood) are definitely not from a stationary Poisson RF, while the last (Pines) point pattern could come from a stationary Poisson RF, even though the $L$-function is outside the 0.9-interval for some $t$.



# Problem 2: Bayesian inversion in Poisson RF
We consider an area of size $(300\times 300)$ m$^2$ containing a pine tree forest, and the actual locations of the pine trees are to be assessed. The pine tree locations are observed from a satellite by remote sensing, and due to partly cloudy weather the observation probability for individual trees vary across the area. We discretize the area into a regular $(30\times30)$-grid $L$ with grid unit size $\Delta_n = 100$m$^2$. The true, but unknown, number of pine trees located in each grid unit is the $n$-vector $k$, with $\{k(\mathbf{x}); \mathbf{x}\in L\}$. The probabilites for observing a tree in each grid unit is represented by the $n$-vector $\{\alpha(\mathbf{x});\mathbf{x}\in L\}$, which vary across the area. We also denote by the $n$-vector $\{d(\mathbf{x});\mathbf{x}\in L\}$ the number of trees observed in each grid unit.

## a)
First, we display the observations and the observation probabilities across the grid. The observation probabilities vary from $0.04$ to $0.67$, and the number of observed trees in each grid unit varies between 0 and 3.
```{r, echo = F, eval = T, out.width = "50%"}
obspines <- read.table("https://www.math.ntnu.no/emner/TMA4250/2018v/Exercise2/obspines.txt")[2:901,]
obsprob <- read.table("https://www.math.ntnu.no/emner/TMA4250/2018v/Exercise2/obsprob.txt")[2:901,]

colnames(obsprob) <- c("x","y","z")
colnames(obspines) <- c("x","y","z")

obspines$x <- as.numeric(obspines$x)
obspines$y <- as.numeric(obspines$y)
obspines$z <- as.numeric(obspines$z)

obsprob$x <- as.numeric(obsprob$x)
obsprob$y <- as.numeric(obsprob$y)
obsprob$z <- as.numeric(obsprob$z)

obsprob$z[obsprob$z>=62] = obsprob$z[obsprob$z>=62]+1
obsprob$z = 0.04 + (obsprob$z-1)*0.01

obsprob = obsprob[order( obsprob[,2], obsprob[,1] ),]
probabilities = list(x = seq(5,295,10), y = seq(5,295,10), z = matrix(obsprob$z, ncol = 30))
image.plot(probabilities, main = "Pine tree observation probabilities")

#interpolation <- interp(obsprob$x, obsprob$y, obsprob$z)
#image.plot(interpolation)

obspines$z = obspines$z - 1
obspines = obspines[order( obspines[,2], obspines[,1] ),]
trees = list(x = seq(5,295,10), y = seq(5,295,10), z = matrix(obspines$z, ncol = 30))

image.plot(trees, main = "Observations of pine trees")
```

We further assume that the observations given the true number of trees are spatially uncorrelated from one grid unit to another. We then specify the likelihood model for the observations. For each grid unit $i$ one has a binomial likelihood model with $d_i \leq k_i$,
$$[d_i|k_i] \sim p(d_i|k_i) = \begin{pmatrix}k_i\\d_i \end{pmatrix} \alpha_i^{d_i}[1-\alpha_i]^{k_i-d_i}.$$
Because the observations are conditionally independent from one node to another, the likelihood model with $\mathbf{d}\leq \mathbf{k}$ is,
$$[\mathbf{d}|\mathbf{k}] \sim p(\mathbf{d}|\mathbf{k}) = \prod_{i=1}^n p(d_i|k_i) = \prod_{i=1}^n \begin{pmatrix} k_i\\d_i\end{pmatrix} \alpha_i^{d_i}[1-\alpha_i]^{k_i-d_i}.$$

## b)
To further specify our model, we assume apriori that the distribution of trees is according to a stationary Poisson RF with model parameter $\lambda_k$. The prior Poisson model is defined on the discretized event count representation $\mathbf{k}\in \mathbb{N}^n_\oplus$ as follows,
$$ \mathbf{k} \sim p(\mathbf{k}) = \prod_{i=1}^n p(k_i) = \prod_{i=1}^n \frac{\pi_k^{k_i}}{k_i!}\times \exp\{-\pi_k\} =  \frac{\pi_k^{\sum_{i}k_i}}{\prod_{i}k_i!}\times \exp\{-n\pi_k\},$$
which is a discretized spatially stationary Poisson count RF with $\{\lambda(\mathbf{x}) = \lambda_k;\mathbf{x}\in D\}$ and $\pi_k = \lambda_k \Delta_n$ and basis-event set $\mathbb{E} = \emptyset$.

## c)
Now we want to estimate the intensity $\lambda_k$ based on the observations with observation probabilities. To estimate the intensity, we start by estimating the event probability $\pi_k$ in the discretized field represented by the $n$-vector $\mathbf{k}$. Hence, the model parameter for $\mathbf{k}$ is $\theta_{pP} = (\pi_k,\emptyset)$. This is assessed through marginal maximum likelihood estimation,
$$\hat \theta_{pP} = \underset{\theta_p}{\operatorname{arg max}}\{\log p(\mathbf{d};\theta_p)\}.$$
By utilizing the joint density of $\mathbf{k}$ and $\mathbf{d}$, and summing each term over $k_i\geq d_i$, we obtain the solution,
$$\hat \pi_k = \frac{n_d}{\boldsymbol \alpha^T \mathbf{i}_n},$$
where $n_d$ is the number of event observations and $\boldsymbol \alpha$ is the observation probabilities. Further, we get the estimated intensity $\hat \lambda_k = \hat \pi_k/\Delta_n$.

Having estimated the intensity, we generate 10 realizations from the prior Poisson event-count model and the associated approximate Poisson event-location realizations. The event-locations are then displayed.

```{r, echo = F, eval = T, out.width = "20%"}
pi_k = sum(obspines$z)/sum(obsprob$z)
lambda_k = pi_k/100

n = length(obspines$z)

generate_event_locations <- function(pi_k,k_zero){
  k <- rpois(n,pi_k)+k_zero
  event_locations <- matrix(NA, ncol = 2, nrow = sum(k))
  count = 1
  for (i in 1:900){
    num = k[i]
    if (num!=0){
      x = runif(num,0,10)
      y = runif(num,0,10)
      event_locations[(count):(count+num-1),1] = x + ((i-1)%%30)*10
      event_locations[(count):(count+num-1),2] = y + ((i-1)%/%30)*10
      }
    count = count + num
    }
  return(event_locations)
  }
x = list()
for (i in 1:10){
  x[[i]] <- generate_event_locations(pi_k,rep(0,n))
  colnames(x[[i]]) = c("x","y")
}
p1 <- ggplot(as.data.frame(x[[1]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 1")
p2 <- ggplot(as.data.frame(x[[2]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 2")
p3 <- ggplot(as.data.frame(x[[3]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 3")
p4 <- ggplot(as.data.frame(x[[4]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 4")
p5 <- ggplot(as.data.frame(x[[5]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 5")
p6 <- ggplot(as.data.frame(x[[6]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 6")
p7 <- ggplot(as.data.frame(x[[7]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 7")
p8 <- ggplot(as.data.frame(x[[8]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 8")
p9 <- ggplot(as.data.frame(x[[9]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 9")
p10 <- ggplot(as.data.frame(x[[10]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 10")
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
```

## d)
Having defined the likelihood model and the prior distribution, we now want to develop the expression for the posterior discretized event-count model. To do this, we first write the joint distribution for the spatial variable $\mathbf{k}$ and the observations $\mathbf{d}$,
\begin{align*}
\begin{bmatrix} \mathbf{k}\\ \mathbf{d} \end{bmatrix} \sim p(\mathbf{k},\mathbf{d}) &= p(\mathbf{d}|\mathbf{k})p(\mathbf{k}) = \prod_{i=1}^n p(k_i|d_i)\prod_{i=1}^n p(k_i) = \prod_{i=1}^n p(d_i,k_i)\\
&= \prod_{i=1}^n \begin{pmatrix} k_i\\ d_i \end{pmatrix} \alpha_i^{d_i}[1-\alpha_i]^{k_i-d_i}\times \frac{\pi_k^{k_i}}{k_i!} \exp\{-\pi_k\}\\
&= \prod_{i=1}^n \frac{[\alpha_i\pi_k]^{d_i}}{d_i!}\times \frac{[(1-\alpha_i)\pi_k]^{k_i-d_i}}{(k_i-d_i)!} \times \exp\{-\pi_k\}. 
\end{align*}
Having arrived at the joint distribution, we note that it is on factorial form, which implies independence of each grid node. Hence, the normalizing constant can be found by summing for each node over $k_i\geq d_i$ to obtain $p(d_i)$. This means that the posterior distribution can be written as,
\begin{align*}
[\mathbf{k}|\mathbf{d}] \sim p(\mathbf{k}|\mathbf{d}) &= \prod_{i=1}^n [p(d_i)]^{-1}\times p(d_i,k_i) = \prod_{i=1}^n p(k_i|d_i)\\
&= \prod_{i=1}^n \frac{[(1-\alpha_i)\pi_k]^{k_i-d_i}}{(k_i-d_i)!} \times \exp\{-(1-\alpha_i)\pi_k\},
\end{align*}
for $\mathbf{k}\in \mathbb{N}_\oplus^n;\mathbf{k}\geq\mathbf{d}$. This is a discretized Poisson count random field with model parameters $\mathbf{\pi}_n = \Big((1-\alpha_1)\pi_k, (1-\alpha_2)\pi_k,\dots,(1-\alpha_n)\pi_k\Big)$ and base event set $\mathbf{k}_n^0 = (d_1,\dots,d_n)$.

To simulate from the associated approximate event-location model, we use Algorithm 7 from [@Omre]. Then we display 10 generated realizations from the posterior approximate event-location model.
```{r, echo = F, eval = T, out.width = "20%"}
x = list()
for (i in 1:10){
  x[[i]] <- generate_event_locations((1-obsprob$z)*pi_k, obspines$z)
  colnames(x[[i]]) = c("x","y")
}

p1 <- ggplot(as.data.frame(x[[1]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 1")
p2 <- ggplot(as.data.frame(x[[2]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 2")
p3 <- ggplot(as.data.frame(x[[3]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 3")
p4 <- ggplot(as.data.frame(x[[4]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 4")
p5 <- ggplot(as.data.frame(x[[5]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 5")
p6 <- ggplot(as.data.frame(x[[6]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 6")
p7 <- ggplot(as.data.frame(x[[7]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 7")
p8 <- ggplot(as.data.frame(x[[8]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 8")
p9 <- ggplot(as.data.frame(x[[9]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 9")
p10 <- ggplot(as.data.frame(x[[10]]), aes(x, y)) + geom_point()+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.y=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=22))  +
    ggtitle("Realization 10")
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
```
If we compare the posterior approximate event-locations with the prior approximate event-locations, we see no clear difference. The posterior and prior realization look very similar. Still, in the posterior realizations we know that the observed points are present, so it will to some extent be different from the prior realizations, but this is hard to observe.

Finally we display three event-count plots. One for the prior model, one for the posterior and one for the observed points.
```{r, echo =F, eval = T, out.width = "33%"}
set.seed(4250)
k <- rpois(n,pi_k)
trees2 = list(x = seq(5,295,10), y = seq(5,295,10), z = matrix(k, ncol = 30))
image.plot(trees2, main = "Realization of prior distribution")

k <- rpois(n,(1-obsprob$z)*pi_k)+obspines$z
trees3 = list(x = seq(5,295,10), y = seq(5,295,10), z = matrix(k, ncol = 30))
image.plot(trees3, main = "Realization of posterior distribution")
image.plot(trees, main = "Observations of pine trees")
```
Here, the differences between the prior and posterior model are more visible. We see clearly that all counts in the observations are present in the posterior model, but not in the prior model, which stems from the fact that the posterior model has the observations as base set $\mathbb{E}$. Even though we see more clear differences in the event-count plots, they are still fairly similar in variablity and overall appearance. 

# Problem 3: Clustered event spatial variables

The Neuman-Scott cluster model (mother-chid model) for event spatial variables is given below. The mother model is usually a stationary Possion RF with intensity $\lambda_M$. Centered at every mother location, $\boldsymbol{x}^M_j$, a set of child events are independently located, with event count given by the pdf $p(k^c)$ and intensity pdf $p(\boldsymbol{x}|\boldsymbol{x}^M_j)$. Censoring is done in order to make sure that the child-event is in $D$. The model can be expressed in terms of the event locations as $\mathbb{X}^{NS}_D = (\boldsymbol{x}_1, \boldsymbol{x}_2, \dots, \boldsymbol{x}_{k_D})$. The locations can also be expressed $\boldsymbol{x}_{ji}$ with $j = 1, \dots, k_D^M$, and $i = 1, \dots, k_{D_j}^c$ where $k_D = \sum_j k^c_{D_j}$, i. e. the i'th child of mother j. We then obtain 
\begin{align*}
\mathbb{X}_D^{NS} &\sim p(\boldsymbol{x}_{ji};j=1,\dots,k_D^M; i=1,\dots,k_{D_j}^c)\\
&= \prod_{j = 1}^{k^M_D} \bigg[ \prod_{i = 1}^{k^c_{D_j}} \frac{I(\boldsymbol{x}_{ji} \in D) p (\boldsymbol{x}_{ji}|\boldsymbol{x}^M_j)} {P(D | \boldsymbol{x}^M_j)} \bigg] \times p(k^c_{D_j} | k^c_j, \boldsymbol{x}^M_j) p(k^c_j) \times \frac{1}{|D|^{k^M_D}}p(k^M_D) \\
&= \prod_{j = 1}^{k^M_D} \bigg[ \prod_{i = 1}^{k^c_{D_j}} \frac{I(\boldsymbol{x}_{ji} \in D) \phi_3(\boldsymbol{x}_{ji}; \boldsymbol{x}^M_j, \sigma^2_c I_3) } {\Phi_3(D | \boldsymbol{x}^M_j, \sigma^2_c I_3)} \bigg]\\
&\times {k^c_l \choose k^c_{D_j}} \Phi_3(D | \boldsymbol{x}^M_j, \sigma^2_c I_3) \Big( 1 - \Phi_3(D | \boldsymbol{x}^M_j, \sigma^2_c I_3) \Big)^{k^c_j - k^c_{D_j}} p(k^c_j) 
\\
&\times \frac{{\lambda_M}^{k^M_D}}{k^M_D!} \exp(-\lambda_M |D|),
\end{align*}
where $k^j_{D_j} \leq k^c_j$. The latter expression corresponds to the case of a mother-child model with a stationary Poission RF as mother model, having intensity $\lambda_M$, and a Gaussian intensity child-model with spread $\sigma^2_c$ where the child set size is given by $p(k^c_j)$. The parameters of this model are $\boldsymbol{\theta}_{pNS} = (\lambda_M, \sigma^2_c, p(.))$. EXPLAIN WHAT IS WHAT

This model is not analytically tractable, but we can sample from it using the following algorithm.
```{r, echo = T, eval = T}
simulate_neuman_scott = function(lambda_m, lambda_c, sigma_c) {
  # simulate one neuman scott process over (0, 1)x(-1, 0)
  
  result = matrix(NA, nrow = 0, ncol = 2)
  # total number of events 
  k = 0
  # number of mothers
  k_m = rpois(1, lambda_m)
  # iterate through the mothers 
  for (j in 1:k_m) {
    # sample uniformly from (0, 1)x(-1, 0) to get mother location
    x_m = c(runif(1, 0, 1), runif(1, -1, 0))
    # sample to get child count 
    k_c = rpois(1, lambda_c)
    # place the children with mother as center 
    for (i in 1:k_c) {
      x_c = mvrnorm(mu = x_m, Sigma = sigma_c * diag(2))
      # check if inside D
      if (x_c[1] > 0 && x_c[1] < 1 && x_c[2] > -1 && x_c[2] < 0) {
        result = rbind(result, x_c)
        k = k + 1
      }
    }
  }
  
  # returns a matrix where the rows are the observations
  return(result)
}
```
This algorithm does however suffer from boundary bias, as we do not get any child contributions from potential mothers outside the domain $D$. 

To get an empirical estimate of the model parameters, we study the redwood tree dataset. There seems to be approximatly $8$ clusters, and a total of $62$ observations. Hence a mother intensity of $8$ and a child intensity of $62/8 = 7.75$ might be fitting. The spread in the clusters corresponds to a $\sigma^2_c$ of approximatly $2.5 \cdot 10^{-3}$, which we have obtained by trial and error. Below the redwood tree data is compared with one realization of this model. 
```{r, echo = F, eval = T, out.width = "50%"}
set.seed(4250)

# display the redwood data
plot(data_redwood, main = "Redwood data", xlab = "x", ylab = "y")

#redwood_df = data.frame(x = data_redwood$x, y = data_redwood$y)

#p = ggplot(data = redwood_df, aes(x=x, y=y)) + 
#  geom_point() + 
#  ggtitle("Redwood data") +
#  theme(plot.title = element_text(hjust = 0.5))
#print(p)

# display the simulated data
sim_mat = simulate_neuman_scott(8, 7.75, 2.5e-3)
plot(sim_mat, main = "Neumann-scott realization", xlab = "x", ylab = "y")
#sim_df = data.frame(x = sim_mat[,1], y = sim_mat[,2])

#p = ggplot(data = sim_df, aes(x=x, y=y)) + 
#  geom_point() + 
#  ggtitle("Neuman-Scott realization") +
#  theme(plot.title = element_text(hjust = 0.5))
#print(p)
```

We now want to justify our choice of parameters using an MCMC test on the $L$-function.
```{r, echo = F, eval = T, out.width = "50%", fig.align = "center", fig.cap = "\\label{fig:NSMonteCarlo}Empirical monte carlo test of our model parameters. We have computed the L-functions from 100 realizations of the Neumann-Scott model, and display these along with the 90$\\%$ interval and the L-function of the Redwood data."}
set.seed(425)
k_redwood <- length(data_redwood$y)
L_matrix_redwood <- matrix(NA, nrow = 100, ncol = length(redwood_L$x))

for (i in 1:100){
  
  # simulate with our parameters
  sim_mat = simulate_neuman_scott(8, 7.75, 2.5e-3)
  df_redwood = data.frame(x = sim_mat[,1], y = sim_mat[,2])
  
  ppinit("redwood.dat")
  ppregion(xl = 0, xu = 1, yl = -1, yu = 0)
  
  L_matrix_redwood[i,] = Kfn(df_redwood, fs = 0.7)$y
}

pred_interval_redwood <- apply(L_matrix_redwood,2,sort)
rownames(L_matrix_redwood) = paste("trial", seq(100), sep="")
colnames(L_matrix_redwood) = paste("x", seq(length(redwood_L$x)), sep="")

dat_redwood = as.data.frame(L_matrix_redwood)
dat_redwood$trial = rownames(L_matrix_redwood)
mdat_redwood = melt(dat_redwood, id.vars="trial")
mdat_redwood$x = as.numeric(gsub("x", "", mdat_redwood$variable))/100*0.7
df_redwood = data.frame(x = seq(0,0.7,length.out = 100), pred = redwood_L$y, lb = pred_interval_redwood[5,], ub = pred_interval_redwood[95,])

ggplot() +
    theme_bw() +
    theme(panel.grid=element_blank()) +
    geom_line(data = mdat_redwood, aes(x=x, y=value, group=trial, col = "Realizations"), size=0.2, alpha=0.1) +
    geom_line(data = df_redwood, aes(x = x, y = pred, col = "Redwood data"), size = 1) + 
    geom_line(data = df_redwood, aes(x = x, y = lb, col = "5th percentile"), size = 1, linetype = "dashed") + 
    geom_line(data = df_redwood, aes(x = x, y = ub, col = "95th percentile"), size = 1, linetype = "dashed") + ylab("L(t)") + xlab("t") + ggtitle("Empirical Monte Carlo test for our choice of model parameters") + scale_color_manual(values = c("blue", "blue", "grey", "red"))
```
From figure \ref{fig:NSMonteCarlo}, we see that the $L$-function of the Redwood data is inside the 90$\%$ interval from the Neumann-Scott model for most of the values of $t$. This justifies our choice of model parameters.

# Problem 4: Repulsive event spatial variables 

The Strauss repulsion model for event spatial variables conditioned on the event count $k_D$ is given below. The event locations $\mathbb{X}^S_D$ given the event count is distributed as follows
$$
\mathbb{X}^S_D | (k_D = k) \sim p(\boldsymbol{x} | k_D = k) \propto \Pi_{i, j \in \{1, \dots, k\}} \exp(-\phi(\boldsymbol{\tau}_{ij}))
$$
with $\boldsymbol{\tau}_{ij} := \boldsymbol{x}_i - \boldsymbol{x}_j$ and where $\phi : \mathbb{R}^n \to \mathbb{R}$ is decreasing with increasing $\| \boldsymbol{\tau}_{ij} \|$. $\phi(.)$ is possibly anisotropic (BETYR DETTE AT DET ER ULIKE BIDRAG FOR FORKJELL I KOMPONENTENE?). A typical form of $\phi(.)$ is 
$$
\phi(\boldsymbol{\tau}) = 
\begin{cases}
  \phi_0, \ &\| \boldsymbol{\tau} \| \in [0, \tau_0] \\
  \phi_0 \exp(-\phi_1 (\| \boldsymbol{\tau} \| - \tau_0)), \ &\| \boldsymbol{\tau} \| > \tau_0
\end{cases}
$$
with model parameters $\boldsymbol{\theta}_{pS|k} = (\tau_0, \phi_0, \phi_1)$ all non-negative. WHAT IS WHAT p. 128. 

This Strauss event RF model is not analytically tractable, but we can simulate from it using the algorithm below
```{r, echo = T, eval = T, out.width = "33%"}
set.seed(4250)

simulate_strauss = function(k, tau0, phi0, phi1, ni = 1000) {
  # simulate one strauss repulsion process over (0, 1)x(0, 1)
  
  # we define the interaction function
  phi = function(tau) {
    dist = sqrt(sum(tau^2))
    if (dist <= tau0) {
      return(phi0)
    } else {
      return(phi0 * exp(-phi1 * (dist - tau0)))
    }
  }
  
  # initalize where initial value has nonzero probability
  result = matrix(0, nrow = k, ncol = 2)
  
  # the algorithm is asymptotically correct. Increase ni to imporove result
  for (i in 1:ni) {
    # choose one index uniformly
    u = sample(1:k, 1)
    
    # generate a location uniformly
    x_p = runif(n = 2)
    
    # compute acceptance probabilty alpha 
    alpha = 0
    for (j in 1:k) {
      alpha = alpha + phi(result[u,] - result[j,]) - phi(x_p - result[j,])
    }
    alpha = exp(-alpha)
    alpha = min(1, alpha)
    
    # accept or reject
    if (runif(1) <= alpha) {
      # accept
      result[u,] = x_p
    }
    
  }
  
  # returns a matrix where the rows are the observations
  return(result)
}

```
This algorithm also suffers from boundary bias, as we ignore possible contributions from events outside the boudary. 

To get an empirical estimate of the model parameters, we study the biological cells dataset. There is a total of $42$ observations. Hence we set $k$ equal to $42$. To obtain estimates of $\tau_0, \phi_0$ and $\phi_1$, we proceed by trial and error. Values of $...$ respectively seems fitting. Iterating $1000$ times seems to work well. Below the biological cells data is compared with one realization of this model. 
```{r, echo = F, eval = T, out.width = "33%"}
set.seed(4250)

# display the biological cell data
cells_df = data.frame(x = data_cells$x, y = data_cells$y)

p = ggplot(data = cells_df, aes(x=x, y=y)) + 
  geom_point() + 
  ggtitle("Biological cells data") +
  theme(plot.title = element_text(hjust = 0.5))

print(p)

# display the simulated data
sim_mat = simulate_strauss(42, 1, 1, 1)

sim_df = data.frame(x = sim_mat[,1], y = sim_mat[,2])

p = ggplot(data = sim_df, aes(x=x, y=y)) + 
  geom_point() + 
  ggtitle("Strauss repulsion model realization") +
  theme(plot.title = element_text(hjust = 0.5))

print(p)

```

We now want to justify our choice of parameters using an MCMC test on the $L$-function. We have done this below.
```{r, echo = F, eval = T, out.width = "33%"}
k_cells <- length(data_cells$y)
L_matrix_cells <- matrix(NA, nrow = 100, ncol = length(cells_L$x))

for (i in 1:100){
  
  sim_mat = simulate_strauss(42, 1, 1, 1)
  sim_df = data.frame(x = sim_mat[,1], y = sim_mat[,2])
  
  ppinit("cells.dat")
  ppregion()
  L_matrix_cells[i,] = Kfn(df_cells,0.7)$y
}

pred_interval_cells <- apply(L_matrix_cells,2,sort)
rownames(L_matrix_cells) = paste("trial", seq(100), sep="")
colnames(L_matrix_cells) = paste("x", seq(length(cells_L$x)), sep="")

dat_cells = as.data.frame(L_matrix_cells)
dat_cells$trial = rownames(L_matrix_cells)
mdat_cells = melt(dat_cells, id.vars="trial")
mdat_cells$x = as.numeric(gsub("x", "", mdat_cells$variable))/100
df_cells = data.frame(x = seq(0,1,length.out = 100), pred = cells_L$y, lb = pred_interval_cells[5,], ub = pred_interval_cells[95,])

ggplot() +
    theme_bw() +
    theme(panel.grid=element_blank()) +
    geom_line(data = mdat_cells, aes(x=x, y=value, group=trial), size=0.2, alpha=0.1) +
    geom_line(data = df_cells, aes(x = x, y = pred), size = 1, col = 'red') + 
    geom_line(data = df_cells, aes(x = x, y = lb), size = 1, col = 'blue', linetype = "dashed") + 
    geom_line(data = df_cells, aes(x = x, y = ub), size = 1, col = 'blue', linetype = "dashed") + ylab("L(t)") + xlab("t") + ggtitle("Empirical Monte Carlo test for our choice of model parameters")
```



# References